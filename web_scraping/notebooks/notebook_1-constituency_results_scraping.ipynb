{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.opera.options import Options\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of constituency names\n",
    "constituency_names = open(\"../data/raw/constituencies.txt\",\"r\").readlines()\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://election.news.sky.com/elections/general-election-2024/\"\n",
    "\n",
    "# Path to your WebDriver (adjust the path as necessary)\n",
    "driver_path = r'../operadriver.exe'\n",
    "\n",
    "# Set up Opera options\n",
    "options = Options()\n",
    "options.binary_location = r\"C:\\Users\\chris\\AppData\\Local\\Programs\\Opera\\opera.exe\"  # Path to the Opera browser executable\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Opera(executable_path=driver_path, options=options)\n",
    "\n",
    "def extract_constituency_name(page_source):\n",
    "    start_h1tag = page_source.find(\"<h1\")\n",
    "    end_h1 = page_source.find(\"</h1>\")\n",
    "    start_h1 = start_h1tag + page_source[start_h1tag:].find(\">\")+1\n",
    "    return page_source[start_h1: end_h1]\n",
    "\n",
    "# Function to scrape data for a given constituency\n",
    "def scrape_constituency_data(idx):\n",
    "    url = f\"{base_url}-{idx}/\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the JavaScript to load\n",
    "    time.sleep(1)  # You might need to adjust the sleep time based on your internet speed and page load time\n",
    "    \n",
    "    page_source = driver.page_source\n",
    "\n",
    "    table_starts = [m.start() for m in re.finditer(\"<table\",page_source)]\n",
    "    table_ends = [m.start()+len(\"</table>\") for m in re.finditer(\"</table>\",page_source)]\n",
    "    html_table = page_source[table_starts[1]:table_ends[1]]\n",
    "\n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_table, 'html.parser')\n",
    "\n",
    "    # Find table headers (column names)\n",
    "    header_row = soup.find('thead').find('tr')\n",
    "    headers = [header.text.strip() for header in header_row.find_all('th')]\n",
    "\n",
    "    # Find all table rows in the tbody\n",
    "    table_rows = soup.find('tbody').find_all('tr')\n",
    "\n",
    "    # List to store JSON objects\n",
    "    json_data = []\n",
    "\n",
    "    # Iterate over each row\n",
    "    for row in table_rows:\n",
    "        # Extract data from each cell in the row\n",
    "        cells = row.find_all('td')\n",
    "        \n",
    "        # Extract text from each span in the cell (skip visually hidden spans)\n",
    "        data = []\n",
    "        for cell in cells:\n",
    "            spans = cell.find_all('span')\n",
    "            cell_data = [span.text.strip() for span in spans if 'u-hide-visually' not in span.get('class', [])]\n",
    "            \n",
    "            # Remove duplicates by converting to a set and back to list\n",
    "            cell_data_unique = list(set(cell_data))\n",
    "            \n",
    "            data.append(' '.join(cell_data_unique))  # Join unique entries into one\n",
    "            \n",
    "        # Create a dictionary using headers and row data\n",
    "        row_data = dict(zip(headers, data))\n",
    "        \n",
    "        # Append the row data dictionary to the list\n",
    "        json_data.append(row_data)    \n",
    "    \n",
    "    constituency_name = extract_constituency_name(page_source)\n",
    "\n",
    "    return json_data, constituency_name\n",
    "\n",
    "# Main loop to scrape data for all constituencies\n",
    "all_data = {}\n",
    "for i in range(650):\n",
    "    data, constituency_name = scrape_constituency_data(i+1)\n",
    "    if data:\n",
    "        all_data[constituency_name] = data\n",
    "    \n",
    "    # Introduce a random delay to mimic human behavior\n",
    "    time.sleep(random.uniform(1,3))\n",
    "\n",
    "    # Optionally, save the data to a JSON file\n",
    "    with open('election_results.json', 'w') as f:\n",
    "        json.dump(all_data, f, indent=4)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "print(\"Scraping completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BART2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
